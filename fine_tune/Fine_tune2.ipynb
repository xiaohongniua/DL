{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 参数是指那些？\n",
    "\n",
    "### 参数其实是个比较泛化的称呼，因为它不仅仅包括一些数字的调整，它也包括了相关的网络结构的调整和一些函数的调整。下面列举一下各种参数：\n",
    "\n",
    "### 1. 数据处理（或预处理）相关参数：\n",
    "#### + enrich data(丰富数据库)\n",
    "#### + feature normalization and scaling（数据泛化处理）\n",
    "#### + batch normalization（BN处理）\n",
    "### 2. 训练过程与训练相关的参数：\n",
    "#### + momentum term（训练动量）\n",
    "#### + BGD, SGD, mini batch gradient descent\n",
    "#### + number of epoch\n",
    "#### + learning rate(学习率）\n",
    "#### + objective function(衰减函数)\n",
    "#### + weight initialization(权值初始化),\n",
    "#### + regularization(正则化相关方法),\n",
    "## 3. 网络相关参数：\n",
    "#### + number of layers,\n",
    "#### + number of nodes,\n",
    "#### + number of filters,\n",
    "#### + classifier(分类器的选择),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 调参的目的以及会出现的问题\n",
    "\n",
    "## 1. 当网络出现训练错误的时候，我们自然需要调整参数。 \n",
    "\n",
    "## 2. 可以训练但是需要提高整个网络的训练准确度。\n",
    "\n",
    "#### 1. 可能没法进行有效地训练，整个网络是错误的,完全不收敛（以下图片全部来源于我的实验，请勿盗用）：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 问题分析理解\n",
    "\n",
    "## 在深度学习领域我上面的三个例子可以说是比较全面的概括了问题的种类：完全不收敛，部分收敛，全部收敛但是误差大。\n",
    "\n",
    "## 下面我们先进行问题分析，然后我再介绍如何具体调参：\n",
    "### 1. 完全不收敛：\n",
    "#### 这种问题的出现可以判定两种原因：1，错误的input data，网络无法学习。 2，错误的网络，网络无法学习. \n",
    "![under_fit](https://github.com/xiaohongniua/DL/blob/master/fine_tune/picture/under_fit.png)\n",
    "### 2. 部分收敛：\n",
    "#### 这种问题的出现是有多种多样的原因的，但是我们可以总结为：1，underfitting。 2， overfitting。\n",
    "![median_fit](https://github.com/xiaohongniua/DL/blob/master/fine_tune/picture/median_fit.png)\n",
    "#### + underfitting和overfitting在深度学习中是两个最最常见的现象，他们的本质是网络分类器的复杂度导致的。\n",
    "#### + 一个过简单的分类器面对一个复杂的数据就会出现underfitting，举个例子：比如一个2类分类器他是无法实现XOR的问题的。一个过复杂的分类器面对一个简单的数据就会出现overfitting。\n",
    "#### 说的更容易理解一点：\n",
    "#### + underfitting就是网络的分类太简单了没办法去分类，因为没办法分类就是没办法学到正确的知识。\n",
    "#### + overfitting就是网络的分类太复杂了以至于它可以学习数据中的每一个信息甚至是错误的信息他都可以学习。\n",
    "#### 如果我们放在实际运用中我们可以这样理解：\n",
    "#### + 我们有两个数据A和B，是两个人的脸的不同环境下10张照片。A我们拿来进行训练，B则是用来测试，我们希望网络可以从A学习人脸的特征从而认识什么是人脸，然后再去判断B是不是人脸。\n",
    "#### + 如果我们使用会导致underfitting的分类器时，这个网络在学习A的时候将根本学不到任何东西，我们本来希望它可以知道A是人脸，但是它却认为这是个汽车（把A分去不正确的label,错误的分类）。\n",
    "#### + 而当我们使用会导致overfitting的分类器时，这个网络在学习A的时候不仅可以知道A是人脸，还认为只有A才是人脸，所以当它去判别B的时候它会认为B不是人脸（过度的学习了过于精确的信息导致无法判别其它数据）。\n",
    "### 3. 全部收敛：\n",
    "#### 这是个好的开始，接下来我们要做的就是微调一些参数。\n",
    "![fit_low_qual](https://github.com/xiaohongniua/DL/blob/master/fine_tune/picture/fit_low_qual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四. 针对问题来调参\n",
    "\n",
    "#### 我在许多其他人的文章看过别人的调参方法的分享和心得，其中有许多人提出使用暴力调参和微调配合使用。这里我不是很同意这种观点，因为所谓的暴力调参就是无规律性的盲目的调整，同时调整多个参数，以寻找一个相对能收敛的结果，再进行微调。这种做法是一种拼运气的做法，运气好的话这个网络的深度和重要的参数是可以被你找到，运气不好的好你可能要花更多的时间去寻找什么是好的结果。而且最终即使有好的结果你自己本身也是不能确认是不是最好的结果。\n",
    "#### 所以再次我建议我们在调参的时候切忌心浮气躁，为了好的实验结果，我们必须一步一步的结果分析，解决问题。\n",
    "#### 下面我们谈谈如和调参：\n",
    "#### 现有的可调参数我已经在（一）中写出了，而这些参数其实有一些是现在大家默认选择的，比如激活函数我们现在基本上都是采用Relu，而momentum一般我们会选择0.9-0.95之间，weight decay我们一般会选择0.005, filter的个数为奇数，而dropout现在也是标配的存在。这些都是近年来论文中通用的数值，也是公认出好结果的搭配。所以这些参数我们就没有必要太多的调整。下面是我们需要注意和调整的参数。\n",
    "### 1. 完全不收敛：\n",
    "#### + 请检测自己的数据是否存在可以学习的信息，这个数据集中的数值是否泛化（防止过大或过小的数值破坏学习）。\n",
    "#### + 如果是错误的数据则你需要去再次获得正确的数据，如果是数据的数值异常我们可以使用zscore函数来解决这个问题（参见我的博客： http://blog.csdn.net/qq_20259459/article/details/59515182 ）。\n",
    "#### + 如果是网络的错误，则希望调整网络，包括：网络深度，非线性程度，分类器的种类等等。\n",
    "### 2. 部分收敛：\n",
    "#### underfitting: \n",
    "#### + 增加网络的复杂度（深度），\n",
    "#### + 降低learning rate，\n",
    "#### + 优化数据集，\n",
    "#### + 增加网络的非线性度（ReLu），\n",
    "#### + 采用batch normalization，\n",
    "#### + overfitting: \n",
    "#### + 丰富数据，\n",
    "#### + 增加网络的稀疏度，\n",
    "#### + 降低网络的复杂度（深度），\n",
    "#### + L1 regularization,\n",
    "#### + L2 regulariztion,\n",
    "#### + 添加Dropout，\n",
    "#### + Early stopping,\n",
    "#### + 适当降低Learning rate，\n",
    "#### + 适当减少epoch的次数，\n",
    "#### 3. 全部收敛：\n",
    "#### + 调整方法就是保持其他参数不变，只调整一个参数。这里需要调整的参数会有：\n",
    "#### + learning rate,\n",
    "#### + minibatch size，\n",
    "#### + epoch，\n",
    "#### + filter size,\n",
    "#### + number of filter,(这里参见我前面两篇博客的相关filter的说明）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. 大的思路和现在的发展\n",
    "\n",
    "#### 其实我们知道现在有许多的成功的网络，比如VGGNet和GoogleNet，这两个就是很划时代的成功。我们也由此可以发现网络设计或者说网络调参的主方向其实只有两种：1. 更深层的网络， 2. 更加的复杂的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出处http://blog.csdn.NET/qq_20259459  和 作者邮箱（ jinweizhi93@gmai.com ）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
